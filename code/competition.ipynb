{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010428,"end_time":"2023-12-15T17:50:57.849629","exception":false,"start_time":"2023-12-15T17:50:57.839201","status":"completed"},"tags":[]},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:43:05.618429Z","iopub.status.busy":"2023-12-17T13:43:05.617659Z","iopub.status.idle":"2023-12-17T13:43:07.313433Z","shell.execute_reply":"2023-12-17T13:43:07.312257Z","shell.execute_reply.started":"2023-12-17T13:43:05.618396Z"},"papermill":{"duration":1.146226,"end_time":"2023-12-15T17:50:59.005971","exception":false,"start_time":"2023-12-15T17:50:57.859745","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import random\n","\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","print(\"##################\")\n","print(\"# IMPORTS LOADED #\")\n","print(\"##################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00989,"end_time":"2023-12-15T17:50:59.026284","exception":false,"start_time":"2023-12-15T17:50:59.016394","status":"completed"},"tags":[]},"source":["# Util_stat.py"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:43:07.316767Z","iopub.status.busy":"2023-12-17T13:43:07.315956Z","iopub.status.idle":"2023-12-17T13:43:07.822729Z","shell.execute_reply":"2023-12-17T13:43:07.821028Z","shell.execute_reply.started":"2023-12-17T13:43:07.316713Z"},"papermill":{"duration":0.757058,"end_time":"2023-12-15T17:50:59.793271","exception":false,"start_time":"2023-12-15T17:50:59.036213","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def haversine(pred, gt):\n","    \"\"\"\n","    Havarsine distance between two points on the Earth surface.\n","\n","    Parameters\n","    -----\n","    pred: numpy array of shape (N, 2)\n","        Contains predicted (LATITUDE, LONGITUDE).\n","    gt: numpy array of shape (N, 2)\n","        Contains ground-truth (LATITUDE, LONGITUDE).\n","\n","    Returns\n","    ------\n","    numpy array of shape (N,)\n","        Contains haversine distance between predictions\n","        and ground truth.\n","    \"\"\"\n","    pred_lat = np.radians(pred[:, 0])\n","    pred_long = np.radians(pred[:, 1])\n","    gt_lat = np.radians(gt[:, 0])\n","    gt_long = np.radians(gt[:, 1])\n","\n","    dlat = gt_lat - pred_lat\n","    dlon = gt_long - pred_long\n","\n","    a = np.sin(dlat/2)**2 + np.cos(pred_lat) * \\\n","        np.cos(gt_lat) * np.sin(dlon/2)**2\n","\n","    d = 2 * 6371 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n","\n","    return d\n","\n","\n","def load_data(csv_path, nb_rows=None):\n","    \"\"\"\n","    Reads a CSV file (train or test) and returns the data contained.\n","\n","    Parameters\n","    ----------\n","    csv_path : String\n","        Path to the CSV file to be read.\n","        e.g., \"train.csv\"\n","    nb_rows : Integer, optional\n","        Number of rows to load from the dataset. If None, load the full dataset.\n","\n","    Returns\n","    -------\n","    data : Pandas DataFrame \n","        Data read from CSV file.\n","    n_samples : Integer\n","        Number of rows (samples) in the dataset.\n","    \"\"\"\n","    data = pd.read_csv(csv_path, index_col=\"TRIP_ID\")\n","\n","    # Sample random rows if nb_rows is specified\n","    if nb_rows is not None:\n","        data = data.sample(n=nb_rows)\n","\n","    return data, len(data)\n","\n","def write_submission(trip_ids, destinations, file_name=\"submission\"):\n","    \"\"\"\n","    This function writes a submission csv file given the trip ids, \n","    and the predicted destinations.\n","\n","    Parameters\n","    ----------\n","    trip_id : List of Strings\n","        List of trip ids (e.g., \"T1\").\n","    destinations : NumPy Array of Shape (n_samples, 2) with float values\n","        Array of destinations (latitude and longitude) for each trip.\n","    file_name : String\n","        Name of the submission file to be saved.\n","        Default: \"submission\".\n","    \"\"\"\n","    n_samples = len(trip_ids)\n","    assert destinations.shape == (n_samples, 2)\n","\n","    submission = pd.DataFrame(\n","        data={\n","            'LATITUDE': destinations[:, 0],\n","            'LONGITUDE': destinations[:, 1],\n","        },\n","        columns=[\"LATITUDE\", \"LONGITUDE\"],\n","        index=trip_ids,\n","    )\n","\n","    # Write file\n","    submission.to_csv(file_name + \".csv\", index_label=\"TRIP_ID\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010027,"end_time":"2023-12-15T17:50:59.863096","exception":false,"start_time":"2023-12-15T17:50:59.853069","status":"completed"},"tags":[]},"source":["# Datasets loading"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011589,"end_time":"2023-12-15T17:50:59.885820","exception":false,"start_time":"2023-12-15T17:50:59.874231","status":"completed"},"tags":[]},"source":["### Train dataset loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:43:56.056520Z","iopub.status.busy":"2023-12-17T13:43:56.056081Z","iopub.status.idle":"2023-12-17T13:44:32.372215Z","shell.execute_reply":"2023-12-17T13:44:32.371120Z","shell.execute_reply.started":"2023-12-17T13:43:56.056488Z"},"papermill":{"duration":5.821132,"end_time":"2023-12-15T17:51:05.718835","exception":false,"start_time":"2023-12-15T17:50:59.897703","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# You can choose the limitation of the Train set, here 1000 rows\n","train_data, len_train_data = load_data(\"train.csv\", nb_rows=1000)\n","\n","print(\"#################\")\n","print(\"# TRAIN LOADED #\")\n","print(\"#################\")\n","print(\"\\n\")\n","\n","train_data"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.01031,"end_time":"2023-12-15T17:51:05.740077","exception":false,"start_time":"2023-12-15T17:51:05.729767","status":"completed"},"tags":[]},"source":["### Test dataset loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:44:35.576878Z","iopub.status.busy":"2023-12-17T13:44:35.576264Z","iopub.status.idle":"2023-12-17T13:44:35.620003Z","shell.execute_reply":"2023-12-17T13:44:35.618882Z","shell.execute_reply.started":"2023-12-17T13:44:35.576834Z"},"papermill":{"duration":0.050313,"end_time":"2023-12-15T17:51:05.800912","exception":false,"start_time":"2023-12-15T17:51:05.750599","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_data, len_data_test = load_data(\"test.csv\")\n","test_dataset_trips_ids = list(test_data.index)\n","\n","print(\"################\")\n","print(\"# TEST LOADED #\")\n","print(\"################\")\n","print(\"\\n\")\n","\n","test_data"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010858,"end_time":"2023-12-15T17:51:05.823458","exception":false,"start_time":"2023-12-15T17:51:05.812600","status":"completed"},"tags":[]},"source":["# Dataset analysis before preprocessing"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010714,"end_time":"2023-12-15T17:51:05.845439","exception":false,"start_time":"2023-12-15T17:51:05.834725","status":"completed"},"tags":[]},"source":["### Dataset polyline lengths analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:44:41.875251Z","iopub.status.busy":"2023-12-17T13:44:41.872142Z","iopub.status.idle":"2023-12-17T13:44:41.885699Z","shell.execute_reply":"2023-12-17T13:44:41.884248Z","shell.execute_reply.started":"2023-12-17T13:44:41.875208Z"},"papermill":{"duration":0.021947,"end_time":"2023-12-15T17:51:05.878562","exception":false,"start_time":"2023-12-15T17:51:05.856615","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def analyze_polyline_lengths(data):\n","    # Storing lengths of each polyline\n","    polyline_lengths = [len(eval(polyline)) for polyline in data['POLYLINE']]\n","\n","    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n","    Q1 = np.percentile(polyline_lengths, 25)\n","    Q3 = np.percentile(polyline_lengths, 75)\n","\n","    # Calculate Interquartile Range (IQR)\n","    IQR = Q3 - Q1\n","\n","    # Calculate minimum and maximum non-outlier thresholds\n","    min_non_outlier = max(Q1 - 1.5 * IQR, 2)\n","    max_non_outlier = Q3 + 1.5 * IQR\n","\n","    # Calculate mean and median of polyline lengths\n","    mean_length = np.mean(polyline_lengths)\n","    median_length = np.median(polyline_lengths)\n","\n","    # Print results\n","    print(\"Non-outlier minimum polyline length:\", min_non_outlier)\n","    print(\"Non-outlier maximum polyline length:\", max_non_outlier)\n","    print(\"Q1:\", Q1)\n","    print(\"Q3:\", Q3)\n","    print(\"IQR:\", IQR)\n","    print(\"Mean length of polyline:\", mean_length)\n","    print(\"Median length of polyline:\", median_length)\n","\n","    # Creating boxplot of polyline lengths\n","    plt.figure(figsize=(10, 6))\n","    plt.boxplot(polyline_lengths)\n","    plt.title(\"Boxplot of Polyline Lengths\")\n","    plt.ylabel(\"Length of Polyline\")\n","    plt.show()\n","\n","    return max_non_outlier"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010677,"end_time":"2023-12-15T17:51:05.900278","exception":false,"start_time":"2023-12-15T17:51:05.889601","status":"completed"},"tags":[]},"source":["##### Train dataset polyline lengths analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:44:46.358077Z","iopub.status.busy":"2023-12-17T13:44:46.356967Z","iopub.status.idle":"2023-12-17T13:44:46.950907Z","shell.execute_reply":"2023-12-17T13:44:46.949728Z","shell.execute_reply.started":"2023-12-17T13:44:46.358030Z"},"papermill":{"duration":81.323494,"end_time":"2023-12-15T17:52:27.234795","exception":false,"start_time":"2023-12-15T17:51:05.911301","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["max_non_outlier_train_data = analyze_polyline_lengths(train_data)\n","\n","print(\"#######################################\")\n","print(\"# TRAIN POLYLINE LENGTH ANALYSIS DONE #\")\n","print(\"#######################################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.01216,"end_time":"2023-12-15T17:52:27.259375","exception":false,"start_time":"2023-12-15T17:52:27.247215","status":"completed"},"tags":[]},"source":["##### Test dataset polyline lengths analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:44:50.081054Z","iopub.status.busy":"2023-12-17T13:44:50.079992Z","iopub.status.idle":"2023-12-17T13:44:50.389759Z","shell.execute_reply":"2023-12-17T13:44:50.388677Z","shell.execute_reply.started":"2023-12-17T13:44:50.081014Z"},"papermill":{"duration":0.357953,"end_time":"2023-12-15T17:52:27.629587","exception":false,"start_time":"2023-12-15T17:52:27.271634","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["max_non_outlier_test_data = analyze_polyline_lengths(test_data)\n","\n","print(\"######################################\")\n","print(\"# TEST POLYLINE LENGTH ANALYSIS DONE #\")\n","print(\"######################################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012521,"end_time":"2023-12-15T17:52:27.655027","exception":false,"start_time":"2023-12-15T17:52:27.642506","status":"completed"},"tags":[]},"source":["# Dataset preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:44:53.601560Z","iopub.status.busy":"2023-12-17T13:44:53.600217Z","iopub.status.idle":"2023-12-17T13:44:53.629835Z","shell.execute_reply":"2023-12-17T13:44:53.628577Z","shell.execute_reply.started":"2023-12-17T13:44:53.601509Z"},"papermill":{"duration":0.039074,"end_time":"2023-12-15T17:52:27.706541","exception":false,"start_time":"2023-12-15T17:52:27.667467","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def preprocess_dataset(data, max_non_outlier):\n","\n","    # Drop 'DAY_TYPE' column (because always of type A)\n","    data = data.drop('DAY_TYPE', axis=1)\n","\n","    # Keep only rows where 'MISSING_DATA' value is False\n","    data = data[data['MISSING_DATA'] == False]\n","\n","    # Drop 'MISSING_DATA' column\n","    data = data.drop('MISSING_DATA', axis=1)\n","\n","    # Keep only rows where 'POLYLINE' is not an empty list\n","    data = data[data['POLYLINE'] != '[]']\n","    \n","    # Replace NaN values in 'ORIGIN_CALL' column with 0\n","    data['ORIGIN_CALL'] = data['ORIGIN_CALL'].fillna(0)\n","\n","    # Replace NaN values in 'ORIGIN_STAND' column with 0\n","    data['ORIGIN_STAND'] = data['ORIGIN_STAND'].fillna(0)\n","    \n","    # Replace categorical values in CALL_TYPE with numerical values\n","    data['CALL_TYPE'] = data['CALL_TYPE'].replace(['A', 'B', 'C'], [0, 1, 2])\n","\n","    for index, row in data.iterrows():\n","\n","        # extract 'POLYLINE' value of current row\n","        polyline = eval(row['POLYLINE'])\n","\n","        # Calculate duration of the ride\n","        duration = len(polyline) * 15\n","        # Create new column for ride duration\n","        data.at[index, 'DURATION'] = duration\n","\n","        polyline_length = len(polyline)\n","\n","        # Create new colum in data for selected longitude and latitude of the points selected\n","\n","         # Assign first point to LONG_0 and LAT_0\n","        data.at[index, 'LONG_0'] = polyline[0][0]\n","        data.at[index, 'LAT_0'] = polyline[0][1]\n","\n","        if polyline_length == 1 or polyline_length == 2:\n","            # Use first point for all if only two points\n","            data.at[index, 'LONG_1'] = polyline[0][0]\n","            data.at[index, 'LAT_1'] = polyline[0][1]\n","            data.at[index, 'LONG_2'] = polyline[0][0]\n","            data.at[index, 'LAT_2'] = polyline[0][1]\n","\n","            # final longitude and latitude from polyline assigned to new END_LONG and END_LATcolumn\n","            data.at[index, 'END_LONG'] = polyline[-1][0]\n","            data.at[index, 'END_LAT'] = polyline[-1][1]\n","\n","        elif polyline_length == 3:\n","            # Use second point for LONG_1, LAT_1, LONG_2, and LAT_2\n","            data.at[index, 'LONG_1'] = polyline[1][0]\n","            data.at[index, 'LAT_1'] = polyline[1][1]\n","            data.at[index, 'LONG_2'] = polyline[1][0]\n","            data.at[index, 'LAT_2'] = polyline[1][1]\n","            \n","            # final longitude and latitude from polyline assigned to new END_LONG and END_LATcolumn\n","            data.at[index, 'END_LONG'] = polyline[-1][0]\n","            data.at[index, 'END_LAT'] = polyline[-1][1]\n","\n","        elif polyline_length == 4:\n","            # Assign second point for LONG_1 and LAT_1\n","            data.at[index, 'LONG_1'] = polyline[1][0]\n","            data.at[index, 'LAT_1'] = polyline[1][1]\n","\n","            # Assign third point for LONG_2 and LAT_2\n","            data.at[index, 'LONG_2'] = polyline[2][0]\n","            data.at[index, 'LAT_2'] = polyline[2][1]\n","\n","            # final longitude and latitude from polyline assigned to new END_LONG and END_LATcolumn\n","            data.at[index, 'END_LONG'] = polyline[-1][0]\n","            data.at[index, 'END_LAT'] = polyline[-1][1]\n","\n","        else:\n","            midpoint_idx = (polyline_length // 2)\n","\n","            # Assign midpoint to LONG_1 and LAT_1\n","            data.at[index, 'LONG_1'] = polyline[midpoint_idx][0]\n","            data.at[index, 'LAT_1'] = polyline[midpoint_idx][1]\n","\n","            # Choose a random point for LONG_2 and LAT_2\n","            random_idx = random.randint(midpoint_idx + 1, polyline_length - 2)\n","            data.at[index, 'LONG_2'] = polyline[random_idx][0]\n","            data.at[index, 'LAT_2'] = polyline[random_idx][1]\n","\n","            if polyline_length > max_non_outlier:\n","                # Choose random point for END_LONG and END_LAT\n","                end_idx = random.randint(random_idx + 1, polyline_length - 1)\n","                data.at[index, 'END_LONG'] = polyline[end_idx][0]\n","                data.at[index, 'END_LAT'] = polyline[end_idx][1]\n","            else:\n","                # Use the last point for END_LONG and END_LAT\n","                data.at[index, 'END_LONG'] = polyline[-1][0]\n","                data.at[index, 'END_LAT'] = polyline[-1][1]\n","\n","    # Drop original 'POLYLINE' column\n","    data = data.drop('POLYLINE', axis=1)\n","\n","    # Convert TIMESTAMP from Unix time to datetime\n","    data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'], unit='s')\n","\n","    # Extract week, day, and quarter\n","    data['WEEK'] = data['TIMESTAMP'].dt.isocalendar().week\n","    data['DAY'] = data['TIMESTAMP'].dt.weekday\n","    data['QUARTER'] = (data['TIMESTAMP'].dt.hour * 4) + \\\n","        (data['TIMESTAMP'].dt.minute // 15)\n","\n","    # Ensure WEEK values are within 1-52\n","    data['WEEK'] = data['WEEK'].clip(1, 52)\n","\n","    # Drop original 'TIMESTAMP' column\n","    data = data.drop('TIMESTAMP', axis=1)\n","\n","    return data"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.01249,"end_time":"2023-12-15T17:52:27.732374","exception":false,"start_time":"2023-12-15T17:52:27.719884","status":"completed"},"tags":[]},"source":["##### Train dataset preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:44:58.048765Z","iopub.status.busy":"2023-12-17T13:44:58.048329Z","iopub.status.idle":"2023-12-17T13:44:58.862528Z","shell.execute_reply":"2023-12-17T13:44:58.861376Z","shell.execute_reply.started":"2023-12-17T13:44:58.048734Z"},"papermill":{"duration":936.511933,"end_time":"2023-12-15T18:08:04.256721","exception":false,"start_time":"2023-12-15T17:52:27.744788","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["preprocessed_train_data = preprocess_dataset(train_data, max_non_outlier_train_data)\n","\n","print(\"######################\")\n","print(\"# TRAIN PREPROCESSED #\")\n","print(\"######################\")\n","print(\"\\n\")\n","\n","preprocessed_train_data"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012734,"end_time":"2023-12-15T18:08:04.282518","exception":false,"start_time":"2023-12-15T18:08:04.269784","status":"completed"},"tags":[]},"source":["##### Test dataset preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:02.118643Z","iopub.status.busy":"2023-12-17T13:45:02.118220Z","iopub.status.idle":"2023-12-17T13:45:02.411706Z","shell.execute_reply":"2023-12-17T13:45:02.410638Z","shell.execute_reply.started":"2023-12-17T13:45:02.118611Z"},"papermill":{"duration":0.343924,"end_time":"2023-12-15T18:08:04.639727","exception":false,"start_time":"2023-12-15T18:08:04.295803","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["preprocessed_test_data = preprocess_dataset(test_data, max_non_outlier_test_data)\n","\n","print(\"#####################\")\n","print(\"# TEST PREPROCESSED #\")\n","print(\"#####################\")\n","print(\"\\n\")\n","\n","preprocessed_test_data"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.013403,"end_time":"2023-12-15T18:08:04.668178","exception":false,"start_time":"2023-12-15T18:08:04.654775","status":"completed"},"tags":[]},"source":["# Dataset longitude & latitude analysis after preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.953651Z","iopub.status.idle":"2023-12-17T13:43:08.956702Z","shell.execute_reply":"2023-12-17T13:43:08.956391Z","shell.execute_reply.started":"2023-12-17T13:43:08.956358Z"},"papermill":{"duration":0.023119,"end_time":"2023-12-15T18:08:04.704906","exception":false,"start_time":"2023-12-15T18:08:04.681787","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def plot_boxplots_for_specific_columns(data):\n","    # Define specific columns to plot\n","    specific_columns = [col for col in data.columns if 'LONG_' in col or 'LAT_' in col or col in ['END_LONG', 'END_LAT']]\n","\n","    # Generate boxplots for each specific column\n","    for column in specific_columns:\n","        plt.figure(figsize=(6, 4))\n","        data[column].plot.box()\n","        plt.title(column)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.013618,"end_time":"2023-12-15T18:08:04.732619","exception":false,"start_time":"2023-12-15T18:08:04.719001","status":"completed"},"tags":[]},"source":["### Train dataset longitude and latitude analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.958677Z","iopub.status.idle":"2023-12-17T13:43:08.959756Z","shell.execute_reply":"2023-12-17T13:43:08.959441Z","shell.execute_reply.started":"2023-12-17T13:43:08.959410Z"},"papermill":{"duration":1.984303,"end_time":"2023-12-15T18:08:06.730596","exception":false,"start_time":"2023-12-15T18:08:04.746293","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plot_boxplots_for_specific_columns(preprocessed_train_data)\n","\n","print(\"################################\")\n","print(\"# TRAIN LONG/LAT ANALYSIS DONE #\")\n","print(\"################################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016933,"end_time":"2023-12-15T18:08:06.764797","exception":false,"start_time":"2023-12-15T18:08:06.747864","status":"completed"},"tags":[]},"source":["### Test dataset longitude and latitude analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.961840Z","iopub.status.idle":"2023-12-17T13:43:08.962906Z","shell.execute_reply":"2023-12-17T13:43:08.962624Z","shell.execute_reply.started":"2023-12-17T13:43:08.962595Z"},"papermill":{"duration":1.2638,"end_time":"2023-12-15T18:08:08.045548","exception":false,"start_time":"2023-12-15T18:08:06.781748","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plot_boxplots_for_specific_columns(preprocessed_test_data)\n","\n","print(\"###############################\")\n","print(\"# TEST LONG/LAT ANALYSIS DONE #\")\n","print(\"###############################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.017981,"end_time":"2023-12-15T18:08:08.082463","exception":false,"start_time":"2023-12-15T18:08:08.064482","status":"completed"},"tags":[]},"source":["# Dataset splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:08.042335Z","iopub.status.busy":"2023-12-17T13:45:08.041520Z","iopub.status.idle":"2023-12-17T13:45:08.048482Z","shell.execute_reply":"2023-12-17T13:45:08.047250Z","shell.execute_reply.started":"2023-12-17T13:45:08.042302Z"},"papermill":{"duration":0.027148,"end_time":"2023-12-15T18:08:08.127705","exception":false,"start_time":"2023-12-15T18:08:08.100557","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def split_dataset(data):\n","    # Separate X = all points except last one and y = last one\n","    X, y = data.drop(['END_LONG', 'END_LAT'], axis=1), data[['END_LONG', 'END_LAT']]\n","    return X, y"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.017904,"end_time":"2023-12-15T18:08:08.164183","exception":false,"start_time":"2023-12-15T18:08:08.146279","status":"completed"},"tags":[]},"source":["### Train dataset splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:13.483281Z","iopub.status.busy":"2023-12-17T13:45:13.482859Z","iopub.status.idle":"2023-12-17T13:45:13.491876Z","shell.execute_reply":"2023-12-17T13:45:13.490683Z","shell.execute_reply.started":"2023-12-17T13:45:13.483248Z"},"papermill":{"duration":0.039414,"end_time":"2023-12-15T18:08:08.221637","exception":false,"start_time":"2023-12-15T18:08:08.182223","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train, y_train = split_dataset(preprocessed_train_data)\n","\n","print(\"##################\")\n","print(\"# TRAIN SPLITTED #\")\n","print(\"##################\")\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.971822Z","iopub.status.idle":"2023-12-17T13:43:08.972898Z","shell.execute_reply":"2023-12-17T13:43:08.972608Z","shell.execute_reply.started":"2023-12-17T13:43:08.972576Z"},"papermill":{"duration":0.043496,"end_time":"2023-12-15T18:08:08.285166","exception":false,"start_time":"2023-12-15T18:08:08.241670","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.974799Z","iopub.status.idle":"2023-12-17T13:43:08.975800Z","shell.execute_reply":"2023-12-17T13:43:08.975530Z","shell.execute_reply.started":"2023-12-17T13:43:08.975502Z"},"papermill":{"duration":0.03474,"end_time":"2023-12-15T18:08:08.339856","exception":false,"start_time":"2023-12-15T18:08:08.305116","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["y_train"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.019088,"end_time":"2023-12-15T18:08:08.379475","exception":false,"start_time":"2023-12-15T18:08:08.360387","status":"completed"},"tags":[]},"source":["### Test dataset splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:17.998728Z","iopub.status.busy":"2023-12-17T13:45:17.998338Z","iopub.status.idle":"2023-12-17T13:45:18.007209Z","shell.execute_reply":"2023-12-17T13:45:18.006014Z","shell.execute_reply.started":"2023-12-17T13:45:17.998699Z"},"papermill":{"duration":0.028837,"end_time":"2023-12-15T18:08:08.427871","exception":false,"start_time":"2023-12-15T18:08:08.399034","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_test, y_test = split_dataset(preprocessed_test_data)\n","\n","print(\"#################\")\n","print(\"# TEST SPLITTED #\")\n","print(\"#################\")\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.980481Z","iopub.status.idle":"2023-12-17T13:43:08.981493Z","shell.execute_reply":"2023-12-17T13:43:08.981232Z","shell.execute_reply.started":"2023-12-17T13:43:08.981204Z"},"papermill":{"duration":0.047008,"end_time":"2023-12-15T18:08:08.495998","exception":false,"start_time":"2023-12-15T18:08:08.448990","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-17T13:43:08.983246Z","iopub.status.idle":"2023-12-17T13:43:08.984204Z","shell.execute_reply":"2023-12-17T13:43:08.983901Z","shell.execute_reply.started":"2023-12-17T13:43:08.983874Z"},"papermill":{"duration":0.033653,"end_time":"2023-12-15T18:08:08.549595","exception":false,"start_time":"2023-12-15T18:08:08.515942","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["y_test"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.019525,"end_time":"2023-12-15T18:08:08.588959","exception":false,"start_time":"2023-12-15T18:08:08.569434","status":"completed"},"tags":[]},"source":["# Model hyperparameters tuning\n","\n","Trying to find the best hyperparameters for kNN, Decision tree and Ridge using GridSearchCV. Linear regression doesn't need hyperparameters, and it's too time-consuming to do so for random forest, so manual tuning is required. Currently we could try: (n_estimators, max_depth, min_samples_split, min_samples_leaf) = (1000, 1000, 10, 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:21.357354Z","iopub.status.busy":"2023-12-17T13:45:21.356924Z","iopub.status.idle":"2023-12-17T13:45:24.193492Z","shell.execute_reply":"2023-12-17T13:45:24.192402Z","shell.execute_reply.started":"2023-12-17T13:45:21.357313Z"},"papermill":{"duration":6378.12142,"end_time":"2023-12-15T19:54:26.730505","exception":false,"start_time":"2023-12-15T18:08:08.609085","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#models_to_tune = [KNeighborsRegressor(), DecisionTreeRegressor(), Ridge()]\n","models_to_tune = [DecisionTreeRegressor(), Ridge()]\n","\n","regression_params = [\n","                        # KNN\n","                        #{\n","                            #'n_neighbors': [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n","                            #'weights': ['uniform', 'distance'],\n","                            #'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n","                        #},\n","                        # DT\n","                        {\n","                            'max_depth': [10, 100, 1000], \n","                            'min_samples_split': [2, 5, 10],\n","                            'min_samples_leaf': [1, 2, 4],\n","                        },\n","                        # Ridge\n","                        {\n","                            'alpha': [0.1, 1, 10, 100, 1000],\n","                        }\n","                    ]\n","\n","print(\"GridSearchCV:\\n\")\n","tuned_models = []\n","tuned_models.append(KNeighborsRegressor(n_neighbors=256, weights='distance', algorithm='auto'))\n","for idx, model in enumerate(models_to_tune):\n","    grid = GridSearchCV(model, param_grid=regression_params[idx], cv=5, verbose=1)\n","    grid.fit(X_train, y_train)\n","    tuned_model = grid.best_estimator_\n","    tuned_models.append(tuned_model)\n","    print(f\"Best parameters for {type(model).__name__}: {grid.best_params_}\")\n","    print(\"\\n\")\n","\n","# add LR and RF manually tuned models\n","tuned_models.append(LinearRegression())\n","tuned_models.append(RandomForestRegressor(n_estimators=1000, max_depth=1000, min_samples_split=10, min_samples_leaf=4, random_state=0))\n","    \n","print(\"###############################\")\n","print(\"# HYPERPARAMETERS TUNING DONE #\")\n","print(\"###############################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.050124,"end_time":"2023-12-15T19:54:26.832505","exception":false,"start_time":"2023-12-15T19:54:26.782381","status":"completed"},"tags":[]},"source":["# Model fitting & prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:30.583613Z","iopub.status.busy":"2023-12-17T13:45:30.582845Z","iopub.status.idle":"2023-12-17T13:45:30.590859Z","shell.execute_reply":"2023-12-17T13:45:30.589729Z","shell.execute_reply.started":"2023-12-17T13:45:30.583577Z"},"papermill":{"duration":0.030928,"end_time":"2023-12-15T19:54:26.898285","exception":false,"start_time":"2023-12-15T19:54:26.867357","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def fit_predict(X_train, y_train, X_test, model):\n","    \"\"\"\n","    Generic function for fitting the model and predicting the destination\n","\n","    Args:\n","        X_train (pandas.core.frame.DataFrame'): data frame of train input\n","        y_train (pandas.core.frame.DataFrame'): data frame of train output\n","        X_test (pandas.core.frame.DataFrame'): data frame of test input\n","        model (_type_): model like DecisionTreeRegressor(max_depth=param, random_state=(RAND_STATE*rand_value))\n","\n","    Returns:\n","        numpy.ndarray': destinations predicted of shape (nb_of_dest, 2)\n","    \"\"\"\n","    \n","    model.fit(X_train, y_train)\n","    \n","    print(\"Fitting done\")\n","    \n","    prediction = model.predict(X_test)\n","    \n","    print(\"Prediction done\")\n","    \n","    # Swap columns (longitude, latitude) -> (latitude, longitude)\n","    prediction = np.stack((prediction[:, 1], prediction[:, 0]), axis=-1)\n","\n","    return prediction"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.020711,"end_time":"2023-12-15T19:54:26.939903","exception":false,"start_time":"2023-12-15T19:54:26.919192","status":"completed"},"tags":[]},"source":["### Make predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:33.842983Z","iopub.status.busy":"2023-12-17T13:45:33.841825Z","iopub.status.idle":"2023-12-17T13:45:41.632226Z","shell.execute_reply":"2023-12-17T13:45:41.630962Z","shell.execute_reply.started":"2023-12-17T13:45:33.842943Z"},"papermill":{"duration":3037.646198,"end_time":"2023-12-15T20:45:04.608413","exception":false,"start_time":"2023-12-15T19:54:26.962215","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["predictions = []\n","\n","# Fit and predict\n","for idx, model in enumerate(tuned_models):\n","    print(\"Starting model nb\", idx)\n","    curr_model_pred = fit_predict(X_train, y_train, X_test, model)\n","    predictions.append(curr_model_pred)\n","    print(\"\\n\")\n","    \n","print(\"####################\")\n","print(\"# PREDICTIONS DONE #\")\n","print(\"####################\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Make csv submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:45:49.500481Z","iopub.status.busy":"2023-12-17T13:45:49.500057Z","iopub.status.idle":"2023-12-17T13:45:49.529179Z","shell.execute_reply":"2023-12-17T13:45:49.528120Z","shell.execute_reply.started":"2023-12-17T13:45:49.500448Z"},"trusted":true},"outputs":[],"source":["# Write submission for each prediction\n","for idx, prediction in enumerate(predictions):\n","    write_submission(test_dataset_trips_ids, prediction, f\"submission_{idx}\")\n","    \n","print(\"############\")\n","print(\"# CSV DONE #\")\n","print(\"############\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.019517,"end_time":"2023-12-15T20:45:04.648819","exception":false,"start_time":"2023-12-15T20:45:04.629302","status":"completed"},"tags":[]},"source":["# Evaluate predictions precision"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T13:46:01.074410Z","iopub.status.busy":"2023-12-17T13:46:01.073971Z","iopub.status.idle":"2023-12-17T13:46:01.167157Z","shell.execute_reply":"2023-12-17T13:46:01.165967Z","shell.execute_reply.started":"2023-12-17T13:46:01.074376Z"},"trusted":true},"outputs":[],"source":["# Load ground truth data\n","ground_truth = pd.read_csv('solutions.csv', index_col='TRIP_ID')\n","\n","# Metrics calculation for each submission file\n","for idx in range(len(predictions)):\n","    submission = pd.read_csv(f\"submission_{idx}.csv\", index_col='TRIP_ID')\n","    submission = submission.loc[ground_truth.index]\n","\n","    gt_public = ground_truth[ground_truth['PUBLIC'] == True]\n","    gt_private = ground_truth[ground_truth['PUBLIC'] == False]\n","\n","    sub_public = submission.loc[gt_public.index]\n","    sub_private = submission.loc[gt_private.index]\n","\n","    # Haversine\n","    hvpublic = haversine(gt_public[['LATITUDE','LONGITUDE']].values, sub_public[['LATITUDE','LONGITUDE']].values)\n","    hvprivate = haversine(gt_private[['LATITUDE','LONGITUDE']].values, sub_private[['LATITUDE','LONGITUDE']].values)\n","\n","    # Mean, Std, MSE, MAE\n","    mean_public = hvpublic.mean()\n","    std_public = hvpublic.std()\n","    mse_public = mean_squared_error(gt_public[['LATITUDE','LONGITUDE']].values, sub_public[['LATITUDE','LONGITUDE']].values)\n","    mae_public = mean_absolute_error(gt_public[['LATITUDE','LONGITUDE']].values, sub_public[['LATITUDE','LONGITUDE']].values)\n","\n","    mean_private = hvprivate.mean()\n","    std_private = hvprivate.std()\n","    mse_private = mean_squared_error(gt_private[['LATITUDE','LONGITUDE']].values, sub_private[['LATITUDE','LONGITUDE']].values)\n","    mae_private = mean_absolute_error(gt_private[['LATITUDE','LONGITUDE']].values, sub_private[['LATITUDE','LONGITUDE']].values)\n","\n","    # Printing results\n","    print(f\"Metrics for submission {idx}:\")\n","    print(f\"Public - Mean: {mean_public}, Std: {std_public}, MSE: {mse_public}, MAE: {mae_public}\")\n","    print(f\"Private - Mean: {mean_private}, Std: {std_private}, MSE: {mse_private}, MAE: {mae_private}\\n\")\n","    \n","print(\"###################################\")\n","print(\"# PREDICTIONS PRECISION EVAL DONE #\")\n","print(\"###################################\")\n","print(\"\\n\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4172286,"sourceId":7222436,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"papermill":{"default_parameters":{},"duration":10451.186812,"end_time":"2023-12-15T20:45:05.605508","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-15T17:50:54.418696","version":"2.4.0"}},"nbformat":4,"nbformat_minor":4}
